{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec15389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from smort.data.text_motion_dataset import TextMotionDataset\n",
    "from smort.models.smort import SMORT\n",
    "from smort.renderer.matplotlib import SingleMotionRenderer\n",
    "from smort.rifke import feats_to_joints\n",
    "from smort.data.collate import length_to_mask\n",
    "from smort.models.text_encoder import TextToEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a73729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smort.data.data_module import InterXDataModule\n",
    "\n",
    "\n",
    "text_motion_dataset = TextMotionDataset(\n",
    "    \"deps/interx/processed/dataset_2k.h5\",\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    text_motion_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=text_motion_dataset.collate_fn,\n",
    "    shuffle=True,\n",
    "    # num_workers=7,\n",
    "    # persistent_workers=True,\n",
    ")\n",
    "\n",
    "data_module = InterXDataModule(\n",
    "    \"deps/interx/processed/dataset_2k.h5\",\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    use_tiny=True,\n",
    "    return_scene=True,\n",
    ")\n",
    "\n",
    "# next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe1a48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cogniveon/src/uos/smort/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name                   | Type              | Params | Mode \n",
      "---------------------------------------------------------------------\n",
      "0 | scene_encoder          | ACTORStyleEncoder | 12.8 M | train\n",
      "1 | text_encoder           | ACTORStyleEncoder | 13.0 M | train\n",
      "2 | motion_decoder         | ACTORStyleDecoder | 19.0 M | train\n",
      "3 | reconstruction_loss_fn | SmoothL1Loss      | 0      | train\n",
      "4 | latent_loss_fn         | SmoothL1Loss      | 0      | train\n",
      "5 | joint_loss_fn          | JointLoss         | 0      | train\n",
      "---------------------------------------------------------------------\n",
      "44.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "44.8 M    Total params\n",
      "179.246   Total estimated model params size (MB)\n",
      "225       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/cogniveon/src/uos/smort/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/cogniveon/src/uos/smort/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23497220639e434ba131eca5ac4bd23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/cogniveon/src/uos/smort/smort/models/modules.py\u001b[0m(89)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     87 \u001b[0;31m        \u001b[0mtoken_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     88 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 89 \u001b[0;31m        \u001b[0maug_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     90 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     91 \u001b[0;31m        \u001b[0;31m# add positional encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "torch.Size([1, 63])\n",
      "> \u001b[0;32m/Users/cogniveon/src/uos/smort/smort/models/modules.py\u001b[0m(89)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     87 \u001b[0;31m        \u001b[0mtoken_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     88 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 89 \u001b[0;31m        \u001b[0maug_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     90 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     91 \u001b[0;31m        \u001b[0;31m# add positional encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "torch.Size([1, 1, 332])\n",
      "torch.Size([1, 1, 332])\n",
      "torch.Size([1, 1, 332])\n",
      "torch.Size([1, 1, 332])\n"
     ]
    }
   ],
   "source": [
    "mean, std = text_motion_dataset.get_mean_std()\n",
    "assert type(mean) == torch.Tensor and type(std) == torch.Tensor\n",
    "model = SMORT(mean, std)\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\", max_epochs=10, fast_dev_run=False, num_sanity_val_steps=0\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd43727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = SMORT.load_from_checkpoint(\n",
    "# #     \"lightning_logs/version_21/checkpoints/epoch=11-step=1200.ckpt\",\n",
    "# # )\n",
    "# text_embeds = TextToEmb(\n",
    "#     \"distilbert/distilbert-base-uncased\",\n",
    "#     device=model.device,\n",
    "# )(\n",
    "#     [\n",
    "#         \"Two people walk towards each other. \"\n",
    "#         \"After they meet, the first person hugs the second person around the \"\n",
    "#         \"shoulders, gently patting his/her back with his/her right hand. \"\n",
    "#         \"Meanwhile, the second person puts his/her arms around the first \"\n",
    "#         \"person's waist and pats his/her waist with his/her right hand.\"\n",
    "#     ]\n",
    "# )\n",
    "# # text_embeds\n",
    "\n",
    "# mask = length_to_mask(text_embeds[\"length\"], device=model.device)\n",
    "# encoded = model.text_encoder(\n",
    "#     {\n",
    "#         \"x\": text_embeds[\"x\"],\n",
    "#         \"mask\": mask,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# dists = encoded.unbind(1)\n",
    "# mu, logvar = dists\n",
    "# latent_vectors = mu\n",
    "# motion = text_motion_dataset.reverse_norm(\n",
    "#     model.motion_decoder(\n",
    "#         {\n",
    "#             \"z\": latent_vectors,\n",
    "#             \"mask\": mask,\n",
    "#         }\n",
    "#     ).squeeze(dim=0)\n",
    "# )\n",
    "\n",
    "# renderer = SingleMotionRenderer(\n",
    "#     colors=(\"red\", \"red\", \"red\", \"red\", \"red\"),\n",
    "# )\n",
    "\n",
    "# renderer.render_animation_single(\n",
    "#     feats_to_joints(torch.from_numpy(motion)).detach().cpu().numpy()\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
